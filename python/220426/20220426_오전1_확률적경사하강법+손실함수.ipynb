{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률적 경사 하강법\n",
    "* 확률적 경사 하강법에서 확률적이란 말은 무작위하게 혹은 랜덤하게의 기술적인 표현입니다. \n",
    "* 경사는 기울기를 의미합니다. \n",
    "* 하강법은 내려가는 방법입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어디서부터 내려올지 선택하기 때문에(랜덤으로 시작점을 뽑는다) 확률적이라고 한다\n",
    "\n",
    "즉, 확률적으로 밑으로 내려오는 것 = 최소값을 찾기 위함 = 오차가 가장 적은 파라미터\n",
    "(인자)를 찾기 위해서이다.\n",
    "\n",
    "딥러닝에서는 일반 미분이 아닌, 편미분을 사용하여 기울기를 찾게된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "산에서 내려온다고 가정하면, 등산로 입구까지 내려갈 때 가장 빠른 길은 경사가 가장 가파른 길이다.\n",
    "\n",
    "= 경사가 가장 가파른 길을 구하기 위해 미분을 사용한다. = 경사 하강법의 개념\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/4-2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보폭 = 학습률(learning ~) 이 너무 크면 원하는 지점(최저점)을 지나갈 수 있다.\n",
    "\n",
    "어떤 보폭이 가장 최적인지 알 수없고, 파라미터 조합을 다 돌려본 뒤 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경사 하강법의 목적 = 훈련 데이터를 통해서 가장 가파른 길을 찾는다!!\n",
    "\n",
    "손실함수의 오차 값이 가장 작아지는 방정식의 계수(가장 가파른 길)가 실제 모형을 가장 잘 표현한다.\n",
    "\n",
    "* 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려간다.\n",
    "이후, 다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택하여 경사를 조금 내려간다.\n",
    "\n",
    "\n",
    "* 이런식으로, 전체 샘플을 모두 사용할 때까지 계속한다. \n",
    "이렇게 해서 답을 찾지 못하면 다시 처음부터 시작한다. (초기화 하는 것이 아니라 이어서 학습시킨다)\n",
    "\n",
    "\n",
    "* 훈련 세트를 모두 사용하는 과정을 에포크(epoch)라고 한다.\n",
    "\n",
    "에포크 = 몇 번 반복해야하는지 정해주어야한다.\n",
    "\n",
    "ex) 문제집 한 권을 다 푸는 것 = 1에포크, 동일한 문제집을 한 번 더 풀면 2에포크\n",
    "\n",
    "에포크가 많을수록 훈련이 잘 되는 것은 맞지만, 시간이 손해볼 수 있다.\n",
    "데이터가 많으면 많을수록 시간이 많이 걸리기 때문에 !! 적절한 에포크 값을 정해야한다.\n",
    "\n",
    "뿐만 아니라 에포크를 너무 늘리게되면 훈련 세트에 과대적합(overfitting) 될 수 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1개의 데이터 셋을 사용하여 내려오는 것보다 여러개를 한 번에 묶어서 사용하면 시간이 더 절약된다.\n",
    "\n",
    "이를 '미니배치 경사 하강법'이라고 하며, 숟가락을 가지고 밥을 먹는 것과 비슷하다.\n",
    "미니배치 경사 하강법은 내부적으로 행렬이 사용된다. 데이터가 행렬로 묶여서 전달된다.\n",
    "\n",
    "* 미니배치 경사 하강법을 많이 사용하며, 시간을 줄이기 위해 사용한다.\n",
    "\n",
    "반대로 전체 샘플을 사용하는 경우 '배치 경사 하강법'이라고 하는데, 밥 한 공기를 한 입에 넣는 것과 비슷하다.(거의 사용하지 않음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실 함수 = 비용 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss(cost) function = 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준이다\n",
    "\n",
    "하이퍼 파라미터 ? ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값은 작을수록 좋지만, 어떤 값이 최솟값인지는 알 수 없다. 최소값에 근접한 값을 찾아 사용해야한다.\n",
    "\n",
    "어떤 손실함수를 사용할지는 우리가 직접 설정해야한다 ㅠㅠ ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수에서 엔트로피 개념을 사용한다\n",
    "\n",
    "실제값과 예측값을 비교하는데, 이 비교했을 때 정보량이 많다 ? = 오답이다\n",
    "ex) 장동건과.. 교수님의 얼굴 ^^,, 나와 전지현의 얼굴 ^_^~\n",
    "\n",
    "* cross-entropy 개념을 사용한다\n",
    "1. 실제 정답과 예측값 사이에 할 말이 많다 = 예측 값이 오답일 확률이 높다.\n",
    "2. 할 말이 별로 없다 = 예측 값이 정답에 매우 근접하다는 의미이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장동건 포스터와 나를 비교했을 때, 할 말이 많이 없어지려면 성형 수술을 하는 것처럼,\n",
    "손실 함수의 계수를 바꿔 오차 값이(정보량) 적어질 때 까지 계속해서 훈련시킨다 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x4OwaSIR50l"
   },
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3z-zKXoRmWB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fish = pd.read_csv('https://bit.ly/fish_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAiJVY9nR1fF"
   },
   "outputs": [],
   "source": [
    "fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()\n",
    "fish_target = fish['Species'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AW6LMW_URpto"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "    fish_input, fish_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RTAwK_DRutj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# https://bskyvision.com/849\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_input)\n",
    "\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSyujXY7sli6"
   },
   "outputs": [],
   "source": [
    "# SGDClassifier = 확률적 경사 하강법의 줄임말\n",
    "# 수식을 모르더라도 확률적 경사 하강법을 사용할 수 있으며, 분류를 할 예정이다.\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1476,
     "status": "ok",
     "timestamp": 1589210002126,
     "user": {
      "displayName": "Haesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsWlS7sKQL-9fIkg3FmxpTMz_u-KDSs8y__P1ngQ=s64",
      "userId": "14935388527648823821"
     },
     "user_tz": -540
    },
    "id": "KofoXhbwR9yu",
    "outputId": "d9148d99-4595-4d82-c6b3-6ec24b67f606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8403361344537815\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# warning 이유 = max_iter 값이 충분하지 않아서 생기는 가이드 라인이다.\n",
    "# 경고를 없애기 위해서는 max_iter = 1000으로 변경하면 된다.\n",
    "sc = SGDClassifier(loss='log', max_iter=1000, random_state=42)\n",
    "sc.fit(train_scaled, train_target)\n",
    "\n",
    "print(sc.score(train_scaled, train_target))\n",
    "print(sc.score(test_scaled, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1470,
     "status": "ok",
     "timestamp": 1589210002126,
     "user": {
      "displayName": "Haesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsWlS7sKQL-9fIkg3FmxpTMz_u-KDSs8y__P1ngQ=s64",
      "userId": "14935388527648823821"
     },
     "user_tz": -540
    },
    "id": "duwA4N3eSUk5",
    "outputId": "3d718c7f-e5bb-483b-bb95-4cb12e7825e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907563025210084\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "# 산에서 내려올 때, 새롭게 학습시키는 것이 아니라 훈련을 이어서 해주는 함수이다.\n",
    "# partial_fit()을 통해 이어서 훈련시키면 성능이 좋아진다.\n",
    "\n",
    "# 이 함수를 이용해서 훈련할 때마다 에포크가 1개씩 늘어난다(이어서 훈련!!)\n",
    "sc.partial_fit(train_scaled, train_target)\n",
    "\n",
    "# 분류이므로 accruracy?\n",
    "# 과대적합, 과소적합 모두 해당하지 않는 것 같다.\n",
    "print(sc.score(train_scaled, train_target))\n",
    "print(sc.score(test_scaled, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEtfnUQhzKO2"
   },
   "source": [
    "## 에포크와 과대/과소적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pt7BHZVZ-dWT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 확률적 경사 하강법 \n",
    "# 손실함수 log ??\n",
    "sc = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "classes = np.unique(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bream', 'Parkki', 'Perch', 'Pike', 'Roach', 'Smelt', 'Whitefish'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-65Gz13tVOP7"
   },
   "outputs": [],
   "source": [
    "for _ in range(0, 300):\n",
    "    sc.partial_fit(train_scaled, train_target, classes=classes)\n",
    "    \n",
    "    train_score.append(sc.score(train_scaled, train_target))\n",
    "    test_score.append(sc.score(test_scaled, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2705,
     "status": "ok",
     "timestamp": 1589210003374,
     "user": {
      "displayName": "Haesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsWlS7sKQL-9fIkg3FmxpTMz_u-KDSs8y__P1ngQ=s64",
      "userId": "14935388527648823821"
     },
     "user_tz": -540
    },
    "id": "V19SzZJ5ZjSI",
    "outputId": "a98e25eb-a217-41d6-b4c4-2460b93b0d10"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_score) # 파란색 = 훈련\n",
    "plt.plot(test_score)  # 주황색 = 테스트\n",
    "plt.show()\n",
    "# 300번 에포크를 진행할 때, test 데이터 그래프를 봤을 때 변화가 없다.\n",
    "# 즉, 약 75회 이후부터는 계속해서 훈련 할 필요가 없다.\n",
    "# 훈련 데이터 결과(파란색이 점점 높아짐)가 올라가고 있는 것 = 과대적합의 가능성 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2972,
     "status": "ok",
     "timestamp": 1589210003647,
     "user": {
      "displayName": "Haesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsWlS7sKQL-9fIkg3FmxpTMz_u-KDSs8y__P1ngQ=s64",
      "userId": "14935388527648823821"
     },
     "user_tz": -540
    },
    "id": "pdp2Ykst1K_I",
    "outputId": "78083114-d489-4325-e1a4-e4fd571d68e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957983193277311\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "# tol = None이 무엇일까? 하다가 시간, 전기를 절약할 수 있게 해주는 조건인 듯 하다.\n",
    "sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)\n",
    "sc.fit(train_scaled, train_target)\n",
    "\n",
    "print(sc.score(train_scaled, train_target))\n",
    "print(sc.score(test_scaled, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1589210003648,
     "user": {
      "displayName": "Haesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsWlS7sKQL-9fIkg3FmxpTMz_u-KDSs8y__P1ngQ=s64",
      "userId": "14935388527648823821"
     },
     "user_tz": -540
    },
    "id": "OL7-y1kgIP4S",
    "outputId": "ebe73575-5bf8-4ba8-b350-224888b9379b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9495798319327731\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "# lost function (손실 함수)의 종류가 다양하게 존재한다.\n",
    "# 이번에는 힌지(hinge) 손실 함수를 사용한 경우이다. \n",
    "# 손실 함수를 바꾸면서 성능이 어떻게 변화하는지 볼 수 있다.\n",
    "# 손실 함수를 모두 돌려보고 어떤 것이 가장 좋은지 판단할 수 있다\n",
    "# 머신러닝 = 돌려보기 전에는 말 하지 말라 !\n",
    "sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)\n",
    "sc.fit(train_scaled, train_target)\n",
    "\n",
    "print(sc.score(train_scaled, train_target))\n",
    "print(sc.score(test_scaled, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGDClassifier in module sklearn.linear_model._stochastic_gradient:\n",
      "\n",
      "class SGDClassifier(BaseSGDClassifier)\n",
      " |  SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      " |  \n",
      " |  Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      " |  \n",
      " |  This estimator implements regularized linear models with stochastic\n",
      " |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      " |  each sample at a time and the model is updated along the way with a\n",
      " |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      " |  (online/out-of-core) learning via the `partial_fit` method.\n",
      " |  For best results using the default learning rate schedule, the data should\n",
      " |  have zero mean and unit variance.\n",
      " |  \n",
      " |  This implementation works with data represented as dense or sparse arrays\n",
      " |  of floating point values for the features. The model it fits can be\n",
      " |  controlled with the loss parameter; by default, it fits a linear support\n",
      " |  vector machine (SVM).\n",
      " |  \n",
      " |  The regularizer is a penalty added to the loss function that shrinks model\n",
      " |  parameters towards the zero vector using either the squared euclidean norm\n",
      " |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      " |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      " |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      " |  online feature selection.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <sgd>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : str, default='hinge'\n",
      " |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      " |      linear SVM.\n",
      " |  \n",
      " |      The possible options are 'hinge', 'log', 'modified_huber',\n",
      " |      'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      " |  \n",
      " |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      " |      'modified_huber' is another smooth loss that brings tolerance to\n",
      " |      outliers as well as probability estimates.\n",
      " |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      " |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      " |      The other losses are designed for regression but can be useful in\n",
      " |      classification as well; see\n",
      " |      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      " |  \n",
      " |      More details about the losses formulas can be found in the\n",
      " |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      " |  \n",
      " |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      " |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      " |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      " |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      " |      not achievable with 'l2'.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Constant that multiplies the regularization term. The higher the\n",
      " |      value, the stronger the regularization.\n",
      " |      Also used to compute the learning rate when set to `learning_rate` is\n",
      " |      set to 'optimal'.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.15\n",
      " |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      " |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      " |      Only used if `penalty` is 'elasticnet'.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether the intercept should be estimated or not. If False, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of passes over the training data (aka epochs).\n",
      " |      It only impacts the behavior in the ``fit`` method, and not the\n",
      " |      :meth:`partial_fit` method.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      The stopping criterion. If it is not None, training will stop\n",
      " |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      " |      epochs.\n",
      " |      Convergence is checked against the training loss or the\n",
      " |      validation loss depending on the `early_stopping` parameter.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether or not the training data should be shuffled after each epoch.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      The verbosity level.\n",
      " |  \n",
      " |  epsilon : float, default=0.1\n",
      " |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      " |      For 'huber', determines the threshold at which it becomes less\n",
      " |      important to get the prediction exactly right.\n",
      " |      For epsilon-insensitive, any differences between the current prediction\n",
      " |      and the correct label are ignored if they are less than this threshold.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      " |      multi-class problems) computation.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  learning_rate : str, default='optimal'\n",
      " |      The learning rate schedule:\n",
      " |  \n",
      " |      - 'constant': `eta = eta0`\n",
      " |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      " |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      " |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      " |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      " |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      " |        training loss by tol or fail to increase validation score by tol if\n",
      " |        early_stopping is True, the current learning rate is divided by 5.\n",
      " |  \n",
      " |          .. versionadded:: 0.20\n",
      " |              Added 'adaptive' option\n",
      " |  \n",
      " |  eta0 : double, default=0.0\n",
      " |      The initial learning rate for the 'constant', 'invscaling' or\n",
      " |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      " |      the default schedule 'optimal'.\n",
      " |  \n",
      " |  power_t : double, default=0.5\n",
      " |      The exponent for inverse scaling learning rate [default 0.5].\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to True, it will automatically set aside\n",
      " |      a stratified fraction of training data as validation and terminate\n",
      " |      training when validation score returned by the `score` method is not\n",
      " |      improving by at least tol for n_iter_no_change consecutive epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'early_stopping' option\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if `early_stopping` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'validation_fraction' option\n",
      " |  \n",
      " |  n_iter_no_change : int, default=5\n",
      " |      Number of iterations with no improvement to wait before stopping\n",
      " |      fitting.\n",
      " |      Convergence is checked against the training loss or the\n",
      " |      validation loss depending on the `early_stopping` parameter.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'n_iter_no_change' option\n",
      " |  \n",
      " |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      " |      Preset for the class_weight fit parameter.\n",
      " |  \n",
      " |      Weights associated with classes. If not given, all classes\n",
      " |      are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      " |      result in a different solution than when calling fit a single time\n",
      " |      because of the way the data is shuffled.\n",
      " |      If a dynamic learning rate is used, the learning rate is adapted\n",
      " |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      " |      this counter, while ``partial_fit`` will result in increasing the\n",
      " |      existing counter.\n",
      " |  \n",
      " |  average : bool or int, default=False\n",
      " |      When set to True, computes the averaged SGD weights accross all\n",
      " |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      " |      an int greater than 1, averaging will begin once the total number of\n",
      " |      samples seen reaches `average`. So ``average=10`` will begin\n",
      " |      averaging after seeing 10 samples.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      " |      Weights assigned to the features.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The actual number of iterations before reaching the stopping criterion.\n",
      " |      For multiclass fits, it is the maximum over every binary fit.\n",
      " |  \n",
      " |  loss_function_ : concrete ``LossFunction``\n",
      " |  \n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |  \n",
      " |  t_ : int\n",
      " |      Number of weight updates performed during training.\n",
      " |      Same as ``(n_iter_ * n_samples)``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.svm.LinearSVC : Linear support vector classification.\n",
      " |  LogisticRegression : Logistic regression.\n",
      " |  Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      " |      ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      " |      penalty=None)``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import SGDClassifier\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> Y = np.array([1, 1, 2, 2])\n",
      " |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      " |  >>> clf = make_pipeline(StandardScaler(),\n",
      " |  ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('sgdclassifier', SGDClassifier())])\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGDClassifier\n",
      " |      BaseSGDClassifier\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseSGD\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Log of probability estimates.\n",
      " |      \n",
      " |      This method is only available for log loss and modified Huber loss.\n",
      " |      \n",
      " |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      " |      and ones, so taking the logarithm is not possible.\n",
      " |      \n",
      " |      See ``predict_proba`` for details.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Input data for prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in\n",
      " |          `self.classes_`.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      This method is only available for log loss and modified Huber loss.\n",
      " |      \n",
      " |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      " |      estimates by simple normalization, as recommended by Zadrozny and\n",
      " |      Elkan.\n",
      " |      \n",
      " |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      " |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      " |      it is necessary to perform proper probability calibration by wrapping\n",
      " |      the classifier with\n",
      " |      :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Input data for prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in `self.classes_`.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      " |      probability estimates\", SIGKDD'02,\n",
      " |      http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      " |      \n",
      " |      The justification for the formula in the loss=\"modified_huber\"\n",
      " |      case is in the appendix B in:\n",
      " |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGDClassifier:\n",
      " |  \n",
      " |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      " |      Fit linear model with Stochastic Gradient Descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      " |          The initial coefficients to warm-start the optimization.\n",
      " |      \n",
      " |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      " |          The initial intercept to warm-start the optimization.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed. These weights will\n",
      " |          be multiplied with class_weight (passed through the\n",
      " |          constructor) if class_weight is specified.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self :\n",
      " |          Returns an instance of self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Perform one epoch of stochastic gradient descent on given samples.\n",
      " |      \n",
      " |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      " |      guaranteed that a minimum of the cost function is reached after calling\n",
      " |      it once. Matters such as objective convergence and early stopping\n",
      " |      should be handled by the user.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Subset of the training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Subset of the target values.\n",
      " |      \n",
      " |      classes : ndarray of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self :\n",
      " |          Returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseSGDClassifier:\n",
      " |  \n",
      " |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGD:\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set and validate the parameters of estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseSGD:\n",
      " |  \n",
      " |  average_coef_\n",
      " |  \n",
      " |  average_intercept_\n",
      " |  \n",
      " |  standard_coef_\n",
      " |  \n",
      " |  standard_intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SGDClassifier)\n",
    "# early_stopping - 시간과 전기를 절약하기 위함^_^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/1RmkE12HtRitdZGliJe4",
   "collapsed_sections": [],
   "name": "4-2 확률적 경사 하강법.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
