{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091c453",
   "metadata": {},
   "source": [
    "# 구조적 스트리밍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ad9a1",
   "metadata": {},
   "source": [
    "* 구조적 스트리밍을 사용하면 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행할 수 있으며, 지연 시간을 줄이고 증분 처리할 수 있습니다. \n",
    "* 구조적 스트리밍은 배치 처리용 코드를 일부 수정하여 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있다는 장점이 있습니다. \n",
    "* 프로토타입을 배치 잡으로 개발한 다음 스트리밍 잡으로 변환할 수 있으므로 개념 잡기가 수월\n",
    "* 다음 예제는 retail 데이터 셋을 사용\n",
    "* 소매 데이터셋에는 특정 날짜와 시간 정보가 포함\n",
    "* 지금은 예제 데이터셋 중 하루치 데이터를 나타내는 by-day 디렉터리의 파일을 사용 \n",
    "-----------\n",
    "* 여러 프로세스에서 데이터가 꾸준하게 생성되는 상황을 상상해보세요\n",
    "* 지금 사용하는 데이터는 소매 데이터이므로 소매점에서 생성된 데이터가 구조적 스트리밍 잡이 읽을 수 있는 저장소로 전송되고 있다고 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d463282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f63ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559176f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfb351",
   "metadata": {},
   "source": [
    "* 시계열 데이터를 다루기 때문에 데이터를 그룹화하고 집계하는 방법을 알아볼 필요가 있음 \n",
    "* 특정 고객(CustomerId로 구분)이 대량으로 구매하는 영업시간을 살펴볼 것 \n",
    "* 예를 들어 총 구매비용 컬럼을 추가하고 고객이 가장 많이 소비한 날을 찾아보자\n",
    "------------\n",
    "* window function는 집계 시에 시계열 컬럼을 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우를 구성\n",
    "* 윈도우는 간격을 통해 처리 요건을 명시할 수 있기 때문에 날짜와 타임스탬프 처리에 유용\n",
    "* 스파크는 관련 날짜의 데이터를 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4072110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   16813.0|{2011-12-01 00:00...|148.54000000000002|\n",
      "|   13102.0|{2011-12-01 00:00...|308.05999999999995|\n",
      "|   15311.0|{2011-11-18 00:00...|266.90999999999997|\n",
      "|   14688.0|{2011-10-18 00:00...|            205.17|\n",
      "|   16950.0|{2010-12-07 00:00...|             172.0|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3a3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b1ba6",
   "metadata": {},
   "source": [
    "### 스트리밍 코드\n",
    "* read 대신 readStream 메서드를 사용하는 것이 큰 차이점\n",
    "* maxFilePerTrigger 옵션을 추가로 지정. 이 옵션을 사용해 한 번에 읽을 파일 수를 설정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e737d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef11b6b",
   "metadata": {},
   "source": [
    "* DataFrame이 스트리밍 유형인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52848fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca642956",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43158062",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[CustomerId#73, window#97], functions=[sum(total_cost#83)])\n",
      "+- StateStoreSave [CustomerId#73, window#97], state info [ checkpoint = <unknown>, runId = 18b350f8-e671-4bbd-8803-c941e564e27b, opId = 0, ver = 0, numPartitions = 5], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[CustomerId#73, window#97], functions=[merge_sum(total_cost#83)])\n",
      "      +- StateStoreRestore [CustomerId#73, window#97], state info [ checkpoint = <unknown>, runId = 18b350f8-e671-4bbd-8803-c941e564e27b, opId = 0, ver = 0, numPartitions = 5], 2\n",
      "         +- *(2) HashAggregate(keys=[CustomerId#73, window#97], functions=[merge_sum(total_cost#83)])\n",
      "            +- Exchange hashpartitioning(CustomerId#73, window#97, 5), ENSURE_REQUIREMENTS, [id=#126]\n",
      "               +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerId#73)) AS CustomerId#73, window#97], functions=[partial_sum(total_cost#83)])\n",
      "                  +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) as double) = (cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) THEN (CEIL((cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) + 1) ELSE CEIL((cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) END + 0) - 1) * 86400000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) as double) = (cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) THEN (CEIL((cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) + 1) ELSE CEIL((cast((precisetimestampconversion(cast(InvoiceDate#71 as timestamp), TimestampType, LongType) - 0) as double) / 8.64E10)) END + 0) - 1) * 86400000000) + 86400000000), LongType, TimestampType)) AS window#97, CustomerId#73, (UnitPrice#72 * cast(Quantity#70 as double)) AS total_cost#83]\n",
      "                     +- *(1) Filter (isnotnull(InvoiceDate#71) AND isnotnull(cast(InvoiceDate#71 as timestamp)))\n",
      "                        +- StreamingRelation FileSource[/data/retail-data/by-day/*.csv], [InvoiceNo#67, StockCode#68, Description#69, Quantity#70, InvoiceDate#71, UnitPrice#72, CustomerID#73, Country#74]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa507f5",
   "metadata": {},
   "source": [
    "* 이 작업 역시 지연 연산이므로 데이터 플로를 실행하기 위해 스트리밍 액션을 호출해야 함 \n",
    "* 스트리밍 액션은 어딘가에 데이터를 채워 넣어야 하므로 count 메서드와 같은 일반적인 정적 액션과는 조금 다른 특성을 가집니다.\n",
    "* 여기서 사용할 스트리밍 액션은 트리거가 실행된 다음 데이터를 갱신하게 될 인메모리 테이블에 데이터를 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ab48aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 05:29:35,555 WARN streaming.ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a286d649-bb8c-4377-84b4-1016e6e0ebe6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "2022-06-08 05:29:35,568 WARN streaming.ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbd100b7bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b814d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|      null|{2011-03-29 00:00...| 33521.39999999998|\n",
      "|      null|{2010-12-21 00:00...|31347.479999999938|\n",
      "|   18102.0|{2010-12-07 00:00...|          25920.37|\n",
      "|      null|{2010-12-10 00:00...|25399.560000000012|\n",
      "|      null|{2010-12-17 00:00...|25371.769999999768|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM customer_purchases\n",
    "  ORDER BY `sum(total_cost)` DESC\n",
    "  \"\"\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "+----------+--------------------+------------------+\n",
    "|CustomerId|              window|   sum(total_cost)|\n",
    "+----------+--------------------+------------------+\n",
    "|      null|{2010-12-21 00:00...|31347.479999999938|\n",
    "|   18102.0|{2010-12-07 00:00...|          25920.37|\n",
    "|      null|{2010-12-10 00:00...|25399.560000000012|\n",
    "|      null|{2010-12-17 00:00...|25371.769999999768|\n",
    "|      null|{2010-12-06 00:00...|23395.099999999904|\n",
    "+----------+--------------------+------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9417e",
   "metadata": {},
   "source": [
    "# Spark 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825f5f0",
   "metadata": {},
   "source": [
    "* 날짜 형식 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb530e",
   "metadata": {},
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3d5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02d4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1ba23",
   "metadata": {},
   "source": [
    "* coalesce 연산자는 파티션 개수를 줄이거나 늘리는 데 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48779cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    "  .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce35e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preppedDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb78c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbc012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd1ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f69ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be89584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8fd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05729a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2171d4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(7,[0,1,4],[2.95,...|\n",
      "|(7,[0,1,4],[2.1,8...|\n",
      "|(7,[0,1,4],[5.95,...|\n",
      "|(7,[0,1,4],[1.65,...|\n",
      "|(7,[0,1,4],[0.42,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformedTraining.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86225fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(7, {0: 2.95, 1: 6.0, 4: 1.0})),\n",
       " Row(features=SparseVector(7, {0: 2.1, 1: 8.0, 4: 1.0})),\n",
       " Row(features=SparseVector(7, {0: 5.95, 1: 2.0, 4: 1.0})),\n",
       " Row(features=SparseVector(7, {0: 1.65, 1: 6.0, 4: 1.0})),\n",
       " Row(features=SparseVector(7, {0: 0.42, 1: 25.0, 4: 1.0}))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.select(\"features\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bc44974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day_of_week_encoded=SparseVector(5, {})),\n",
       " Row(day_of_week_encoded=SparseVector(5, {})),\n",
       " Row(day_of_week_encoded=SparseVector(5, {})),\n",
       " Row(day_of_week_encoded=SparseVector(5, {})),\n",
       " Row(day_of_week_encoded=SparseVector(5, {}))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.select(\"day_of_week_encoded\").tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17fce81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    "  .setK(20)\\\n",
    "  .setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca863f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e86284",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282b92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61770c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kmModel.transform(transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0612ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483146b",
   "metadata": {},
   "source": [
    "#### 실루엣 \n",
    "* 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 표시\n",
    "* 개별 데이터가 가지는 군집화 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9448e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "silhouette = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82192f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9335330049792616\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e12faff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[3.61878368 5.72170058 0.19585629 0.19391248 0.18010294 0.17205723\n",
      " 0.14921318]\n",
      "[1.0400e+00 7.4215e+04 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00]\n",
      "[ 1.6670865e+04 -1.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n",
      "[ 1.0400e+00 -7.4215e+04  0.0000e+00  1.0000e+00  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00]\n",
      "[1.37047872e+03 2.12765957e-02 1.48936170e-01 1.70212766e-01\n",
      " 4.25531915e-01 4.25531915e-02 2.12765957e-01]\n",
      "[ 3.897e+04 -1.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "  1.000e+00]\n",
      "[ 5.43415e+03 -1.00000e+00  0.00000e+00  1.25000e-01  3.75000e-01\n",
      "  0.00000e+00  5.00000e-01]\n",
      "[8.45308642e-01 1.10427160e+03 2.46913580e-01 2.34567901e-01\n",
      " 1.11111111e-01 1.97530864e-01 1.60493827e-01]\n",
      "[1.16302932e+00 3.77254072e+02 3.01302932e-01 1.98697068e-01\n",
      " 1.33550489e-01 1.93811075e-01 1.43322476e-01]\n",
      "[ 0.000e+00 -5.368e+03  0.000e+00  0.000e+00  0.000e+00  1.000e+00\n",
      "  0.000e+00]\n",
      "[ 1.46892028 83.37677338  0.21217828  0.22284997  0.13082235  0.21456372\n",
      "  0.15204018]\n",
      "[ 7.5000e-03 -9.4045e+03  2.5000e-01  7.5000e-01  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00]\n",
      "[ 1.3524695e+04 -5.0000000e-01  0.0000000e+00  1.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "[5.13333333e-01 4.06866667e+03 3.33333333e-01 3.33333333e-01\n",
      " 0.00000000e+00 0.00000000e+00 3.33333333e-01]\n",
      "[ 7.385808e+03 -6.000000e-01  0.000000e+00  8.000000e-01  2.000000e-01\n",
      "  0.000000e+00  0.000000e+00]\n",
      "[ 9.28571429e-01 -2.49885714e+03  0.00000000e+00  1.42857143e-01\n",
      "  5.71428571e-01  2.85714286e-01  0.00000000e+00]\n",
      "[ 3.78913043e-01 -9.93804348e+02  1.73913043e-01  2.39130435e-01\n",
      "  1.95652174e-01  2.39130435e-01  1.52173913e-01]\n",
      "[9.74117647e-01 2.41200000e+03 2.35294118e-01 1.76470588e-01\n",
      " 2.35294118e-01 1.17647059e-01 1.76470588e-01]\n",
      "[2.52006398e+02 8.76957494e-01 1.85682327e-01 2.93064877e-01\n",
      " 2.34899329e-01 1.27516779e-01 1.52125280e-01]\n",
      "[0.000e+00 5.568e+03 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00]\n"
     ]
    }
   ],
   "source": [
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c601c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
