# ec2 구동 
# 하둡 시작 
# 스파크 시작(pyspark)
pyspark --master yarn --num-executors 3
pyspark --master yarn --num-executors 3 &


# hdfs 에서 파일 올리기 
# 제공 된 csv 파일을 
# hdfs의 /data/flight-data/csv 경로로 업로드 하기!(10시 40분까지)
# 폴더 생성 
hdfs dfs -mkdir -p /data/flight-data/csv
# 파일 업로드 (현재 폴더에 csv파일이 있다는 ... )
hdfs dfs -put *.csv /data/flight-data/csv/

hdfs dfs -mkdir -p /data/retail-data/
hdfs dfs -put *.by-day /data/retail-data/


Q: 근데 pyspark라고 실행하면 jupyter가 실해되도록 했는데 그러면 
   그냥 jupyter notebook -- master yarn.... 이런식으로 실행하도 되는건가요??

Q : 환경설정에서 jupyter 가 실행되도록 하는 설정을 안하고 그냥 pyspark 실행하면 어떻게 되나요??

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_332)
Type in expressions to have them evaluated.
Type :help for more information.
>> 
   

# chapter 3 노트북 열어주시고...
# by-day에 있는 csv파일을 
# hdfs -> /data/retail-data/by-day/ 파일 업로드 


Q : SparkSQL하고 DataFrame 둘 중에 뭐가 더 좋을까요? 
A : https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547







purchaseByCustomerPerHour.writeStream\
    .format("memory")\
    .queryName("customer_purchases")\
    .outputMode("complete")\
    .start()
    
    
    
Q : 실시간이 무슨의미인지 모르겠어요 저희가 데이터가 실시간으로 넣고있는게 아닌데 실시간으로 처리한다는게 무슨말인가요?
A : readStream 메서드를 사용해서 마치 실시간으로 데이터를 받는 것처럼 파일을 1개씩 계속 읽어들인다. 
    - 실시간 환경 흉내!!!
    
    
# 리눅스에서 실시간 메모리 확인 
# 단위를 M단위로 100번 실시간 보여주기 
free -h -c 100


# 리눅스에서 프로세스 활동을 확인 
top 
# 종료할때는 q 

# 터미널이 종료가 되어도 프로그램을 계속 실행하고 싶을때 
nohup jupyter notebook & 


# 머신러닝 실행 코드 셀 하나 추가 
staticDataFrame = spark.read.format("csv")\
  .option("header", "true")\
  .option("inferSchema", "true")\
  .load("/data/retail-data/by-day/*.csv")
  
  
pyspark --master yarn --num-executors 3

Q : 처음에 spark 실행 안해도 되는건가요? spark 실행하는 이유는 웹사이트에서 spark job 확인하기 위해서 실행하는건가요?


Q : VectorAssembler 얘도 단순히 합치는게 아닌거 같은데 분석을 위해 처리하는건가요?
