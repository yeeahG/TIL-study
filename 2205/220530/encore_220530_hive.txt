---------------------------jupyter--------------------------
# 필요한 라이브러리 import
from bs4 import BeautifulSoup
from datetime import date,datetime, timedelta
import re, os, requests
import pandas as pd


# 브라우저 사용해서 접속하는 것처럼 하기 위해서 headers 입력 
head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Edg/86.0.622.38'}


url = "https://n.news.naver.com/mnews/article/119/0002608171?sid=101"

r = requests.get(url, headers=head)

bs = BeautifulSoup(r.text, 'lxml')

bs.find("div", id="newsct_article").text.strip()

# 정규식
# [가-힣]  -> 모든 한글 2~4 그리고 공백(0개이상 0 ~ ) 기자 
reporter = re.compile("[가-힣]{2,4}\s*기자")
text = bs.find("div", id="newsct_article").text.strip()
re.sub(reporter, "", text)

# 정규식 테스트
re.sub(reporter, "", "손흥민       기자")

# 이메일 정규식 
email   = re.compile("[\w._%+-]+@[\w.-]+\.[a-zA-Z]{2,4}")
re.findall(email, "하하하 afas@gmail.com 입니다.")


# 기자 이름과 이메일 정규식으로 삭제 
re.sub(reporter, "", re.sub(email, "", "오상헌 기자 (bborirang@mt.co.kr)"))

######################################
reporter = re.compile("[가-힣]{2,4}\s*기자")
email   = re.compile("[\w._%+-]+@[\w.-]+\.[a-zA-Z]{2,4}")
reporter2 = re.compile("[가-힣]{2,4}\s*특파원")

text = bs.find("div", id="newsct_article").text.strip()

text = re.sub(email, "", text)
text = re.sub(reporter, "", text)    
text = re.sub(reporter2, "", text) 
######################################


def article(date_, url):
    reporter = re.compile("[가-힣]{2,4}\s*기자")
    email   = re.compile("[\w._%+-]+@[\w.-]+\.[a-zA-Z]{2,4}")
    r = requests.get(url, headers= head)
    bs = BeautifulSoup(r.text)
    try:
        rt = bs.find("div", id="newsct_article")
        text = rt.text.strip()
    except:
        return  
    
    try:
        text = text[:text.find(rt.select_one("a").text)]
    except:
        pass    
    text = re.sub(email, "", text)
    text = re.sub(reporter, "", text)    
    if not os.path.isdir(date_):
        os.mkdir(date_)
    f = open("./" + date_ + "/" + url.split("?")[0].split("/")[-1] + ".txt", "w", encoding='utf-8')
    f.write(text)
    f.close()
    
    
def naver_news(start_date = None, end_date = None):
    sid2_cate = [259, 258, 261, 771, 260, 310, 263]
    url = "https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid2={}&sid1=101&date={}&page={}"
    head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Edg/86.0.622.38'}
    for date_ in [str(x).replace("-", "")[:8] for x in pd.date_range(start_date, end_date)]:
        for cate in sid2_cate:
            r= requests.get(url.format(cate, date_, 200 ), headers= head)
            bs = BeautifulSoup(r.text, "lxml")
            #print(x , end=' : ')
            last_page = bs.find("div", class_='paging').find("strong").text
            #print ("last page:{}".format(last_page)
            for page_num in range(1, int(last_page)+1):
                r2= requests.get(url.format(cate, date_, page_num ), headers= head)
                bs2 = BeautifulSoup(r2.text, 'lxml')
                for x in bs2.find("div", class_="list_body newsflash_body").findAll("dt"):
                    article(date_, x.a['href'])

naver_news('2022-05-23', '2022-05-30')
#  20220523, 20220524.... 20220530
                    

---------------------------jupyter-----------------end--------

--------------aws, putty---------------
##################
# 인스턴스 4개 실행 
# client에 뉴스 기사 zip 파일 업로드 
# zip 파일 압축을 해제 



# client에서 unzip  설치 
sudo yum install unzip -y

su hadoop
cd ~/naver
unzip 20220523.zip


cd ~
vim start.sh
# 아래 내용을 입력 
ssh namenode start-dfs.sh
ssh secondnode start-yarn.sh
ssh namenode mr-jobhistory-daemon.sh start historyserver

# 저장하고 나온 뒤 
. start.sh


# 프로세스 확인 
vim check.sh 


echo "=========namenode==============="
ssh namenode jps
echo "=========secondnode==================="
ssh secondnode jps
echo "=========datanode3==================="
ssh datanode3 jps

# 저장 하고 나온 뒤에 
. check.sh



# hdfs에 naver 폴더 만들기 
hdfs dfs -mkdir /naver

# 확인 
hdfs dfs -ls /

# 데이터 넣기 
cd ~/naver
hdfs dfs -put *.txt /naver


hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep /naver /naver_out3 '[가-힣]+'


# 하둡 구동 정지 
vim stop.sh

ssh secondnode stop-yarn.sh
ssh namenode mr-jobhistory-daemon.sh stop historyserver
ssh namenode stop-dfs.sh

# 저장하고 나온뒤 
. stop.sh


# 다시 재가동 
. start.sh



# mr 브라우저 확인하기 
# secondnode public ip
# 접속이 안되면.... 내 보안그룹에서 8088 port가 개방되어 있는지 확인 
http://15.164.221.186:8088/cluster


# 인스턴스 교체 하신분들은... 
# client 다시 접속 (hadoop 계정으로)
su hadoop 
cd ~ 
. start.sh 


hdfs dfs -cat /naver_out3/part-r-00000


# aws의 vCPU 제한 풀기 
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html#vcpu-limits-request-increase
  

  
# client에 hive 다운로드 
cd ~
wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz


# hive 파일 압축 풀기
tar xvzf ./apache-hive-3.1.2-bin.tar.gz

# apache-hive-3.1.2-bin -> hive 
mv apache-hive-3.1.2-bin ./hive

########################################################################
# namenode로 이동
ssh namenode


# mysql 설치 
sudo yum install mariadb-server -y

# mariadb를 시작 프로그램 등록 하고 바로 실행 
sudo systemctl enable --now mariadb
sudo systemctl status mariadb

# mariadb 설정
mysql_secure_installation

# root 패스워드 설정하기  -> Y
 # 비밀번호 설정하세요 (123)
# 익명의 사용자 삭제 -> Y
# Disallow root login remotely? -> N
# Remove test database and access to it?  -> Y
# Reload privilege tables now? [Y/n] Y

# mysql 접속하기 
mysql -u root -p
# 패스워드 입력

# hive를 위한 db를 생성
create database hivedb;
# hiveuser 계정 생성 
create user hiveuser@localhost identified by 'hivepw';

# 접속 권한 부여 
grant all privileges on hivedb.* to hiveuser@localhost;
grant all privileges on hivedb.* to hiveuser@'client' identified by 'hivepw';


flush privileges;

##################################################################
# client로.... 

vim ~/.bashrc
export HIVE_HOME=/home/hadoop/hive

source ~/.bashrc
echo $HIVE_HOME


# 다운로드 받은 2개의 SH XML 파일을 
# /home/hadoop/hive/conf 경로로 filezillar 업로드 

cd ~/hive/lib

cd $HIVE_HOME/lib
rm guava-19.0.jar

# 공유폴더에서 다운로드 받은 jar파일 /lib 폴더에 업로드 
# guava와 mysql jar 업로드 
# /home/hadoop/hive/lib




# hdfs 작업 
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -mkdir /tmp
hdfs dfs -chmod g+w /tmp

#### client 
source ~/.bashrc 
schematool -initSchema -dbType mysql

### Initialization script completed
### schemaTool completed

# namenode로 이동 
mysql -u root -p
use hivedb;
show tables;

# client 복귀 
hive 


# hive table 생성 
CREATE TABLE FILES (line STRING);

LOAD DATA INPATH '/naver/*' OVERWRITE INTO TABLE FILES;

SELECT * FROM FILES LIMIT 5;


CREATE TABLE word_counts AS 
SELECT word, count(1) AS count FROM
(SELECT explode(split(line, ' ')) AS word FROM FILES) w
GROUP BY word 
ORDER BY word;

SELECT * FROM word_count ORDER BY count;


